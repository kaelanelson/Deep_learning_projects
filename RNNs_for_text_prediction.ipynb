{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Otw7II_IKWZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c9c0dde1-4f16-4d94-8c3b-8ef791b92d19"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch \n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 35kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5956a000 @  0x7fc2843022a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vWTnzyug2KAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GUArg13Kkpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "46702da3-8415-42d4-8f30-73ae6ab76feb"
      },
      "cell_type": "code",
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-13 05:42:26--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 52.20.136.189, 52.55.184.84, 34.200.202.18, ...\n",
            "Connecting to piazza.com (piazza.com)|52.20.136.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2018-10-13 05:42:26--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 143.204.98.78, 143.204.98.189, 143.204.98.56, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|143.204.98.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "\r./text_files.tar.gz   0%[                    ]       0  --.-KB/s               \r./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2018-10-13 05:42:26 (38.5 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 15.3MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.0.22\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OHWUBX_OKtOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e03129ed-24d8-4577-f757-638a6f2bb724"
      },
      "cell_type": "code",
      "source": [
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "import re\n",
        " \n",
        "import pdb\n",
        " \n",
        "all_characters = string.printable # used for one hot encoding\n",
        "n_characters = len(all_characters)\n",
        " \n",
        "file1 = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file2 = unidecode.unidecode(open('./text_files/alma.txt').read())\n",
        "file3 = unidecode.unidecode(open('./all_tswift_lyrics.txt').read()) # taylor swift lyrics\n",
        "file4 = unidecode.unidecode(open('./sense_and_sensibility.txt').read()) # first 5 chapters of sense and sensibility\n",
        "file1_len = len(file1)\n",
        "print('file_len =', file_len)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file_len = 2579888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4S5i6NQyKv4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "6974555c-c0ce-44b5-a020-de6c8a33629f"
      },
      "cell_type": "code",
      "source": [
        "chunk_len = 200 # chunks are like batches\n",
        " \n",
        "def random_chunk(file, file_len):\n",
        "    start_index = random.randint(0, file_len - chunk_len)\n",
        "    end_index = start_index + chunk_len + 1\n",
        "    return file[start_index:end_index]\n",
        " \n",
        "print(random_chunk(file1, file1_len))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The three others were far away in the North. In \n",
            "the house of Elrond it is told that they were at Ann(r)minas, and Amon Syl, \n",
            "and Elendil's Stone was on the Tower Hills that look towards Mithlond in th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hMkU_6xGKx5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "919a2612-75b6-4625-dda9-0624b94b0924"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        tensor[c] = all_characters.index(string[c])\n",
        "    return Variable(tensor) # tells you location of character in all_characters\n",
        "  \n",
        "print(char_tensor('abcDEF'))\n",
        "\n",
        "def random_training_set(file):    \n",
        "    chunk = random_chunk(file, len(file))\n",
        "    inp = char_tensor(chunk[:-1])\n",
        "    target = char_tensor(chunk[1:])\n",
        "    return inp, target"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tsQw3zvzMQMz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# RNN and GRU"
      ]
    },
    {
      "metadata": {
        "id": "THXQ-GkjK8FX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        " \n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, n_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    \n",
        "    self.rt = nn.Sequential(nn.Linear(self.input_size + self.hidden_size,  self.hidden_size, bias=True), nn.Sigmoid())\n",
        "    self.zt = nn.Sequential(nn.Linear(self.input_size + self.hidden_size, self.hidden_size, bias=True), nn.Sigmoid())\n",
        "    self.nt = nn.Sequential(nn.Linear(self.input_size + self.hidden_size, self.hidden_size, bias=True), nn.Tanh())\n",
        "  \n",
        "  def forward(self, input, hidden):\n",
        "    # concatenates input and hidden layer \n",
        "    temp = torch.cat((input, hidden), dim=2)\n",
        "    r = self.rt(temp)\n",
        "    z = self.zt(temp)\n",
        "    \n",
        "    # concatenates input and r*hidden layer \n",
        "    rh = torch.cat((input, r*hidden), dim=2)\n",
        "    n = self.nt(rh)\n",
        "    h = (1-z)*n +z*hidden\n",
        "    \n",
        "    return h, h\n",
        "    \n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        " \n",
        "        # encode using embedding layer\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    \n",
        "        # set up GRU passing in number of layers parameter (nn.GRU)\n",
        "        self.gru = GRU(hidden_size, hidden_size, n_layers)\n",
        "      \n",
        "        # decode output\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        " \n",
        "    def forward(self, input, hidden):\n",
        "        # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "        # of the GRU\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(embedded)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        \n",
        "        # return output and hidden\n",
        "        return output, hidden\n",
        " \n",
        "    def init_hidden(self):\n",
        "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oxznMkpFMLbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train and Predict"
      ]
    },
    {
      "metadata": {
        "id": "0xMhA_mJK-Gz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(inp, target):\n",
        "    ## initialize hidden layers, set up gradient and loss \n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[c], hidden) # run the forward pass of your rnn with proper input\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        " \n",
        "    ## calculate backwards loss and step the optimizer (globally)\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        " \n",
        "    return loss.item() / chunk_len\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "    ## initialize hidden variable, initialize other useful variables \n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        " \n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        " \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden) # run your RNN/decoder forward on the input\n",
        " \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        " \n",
        "        ## get character from your list of all characters, add it to your output str sequence, set input\n",
        "        ## for the next pass through the model\n",
        "    \n",
        "        predicted += all_characters[top_i.item()]\n",
        "        inp = top_i      \n",
        "\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nte4bJMlL7md",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lord of the Rings"
      ]
    },
    {
      "metadata": {
        "id": "KJrMHKkgK3zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        },
        "outputId": "f39fbece-1ca6-4a26-dc36-aabdcbcef2c5"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_epochs = 2000\n",
        "print_every = 100\n",
        "plot_every = 10\n",
        "hidden_size = 100\n",
        "n_layers = 1\n",
        "lr = 0.005\n",
        " \n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set(file1))       \n",
        "    loss_avg += loss_\n",
        " \n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate('Wh', 100), '\\n')\n",
        " \n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20.1297664642334 (100 5%) 2.4010]\n",
            "Whand saand \n",
            "\n",
            "\n",
            "thand Tto smos raf ing. wererer ouh \n",
            "\n",
            "\n",
            "\n",
            "ok me ruthe renoned. fe storeis end and wi ente \n",
            "\n",
            "[40.125828981399536 (200 10%) 2.0951]\n",
            "Wh themily sall mater. 'Bo forrmorrted muclinddaved and the cam shall crathe andonln be oust frondi'n  \n",
            "\n",
            "[60.323487281799316 (300 15%) 1.9893]\n",
            "Whe fat tro Hrode gromtalpen And went, sowen \n",
            "waco rim not bampit frorgo wean tubut wile, wive not ot  \n",
            "\n",
            "[80.403480052948 (400 20%) 2.1765]\n",
            "Wh gast Last of take wed, and and to with me trey and frame a were ney the \n",
            "frean. \n",
            "\n",
            "\n",
            "There of anr you \n",
            "\n",
            "[100.50981187820435 (500 25%) 1.8353]\n",
            "Wh with is ner \n",
            "\n",
            "his in \n",
            "\n",
            "the biedorn and the he sleet, Smaws of hill tien will in thingth!' The clowl \n",
            "\n",
            "[120.60929751396179 (600 30%) 1.9324]\n",
            "Wh ligh his we hither thoued on the stroud see? Frod of the said they in befut if the caming and bene  \n",
            "\n",
            "[140.72434329986572 (700 35%) 1.6532]\n",
            "Whers in \n",
            "the doows of thell so of the \n",
            "call may not had must on the pid det. Tink, not to a \n",
            "leed. Bu \n",
            "\n",
            "[160.7709355354309 (800 40%) 1.6807]\n",
            "Whould and the soon the ssaid herry but intanted the mastor with book the was that this soind windted  \n",
            "\n",
            "[180.98124837875366 (900 45%) 1.7697]\n",
            "Whorst your a bight all rodo the ownward. 'Weth going are shave by, and the mlat, hjolil, are in the b \n",
            "\n",
            "[201.11536169052124 (1000 50%) 2.0437]\n",
            "Whers be of the bolding of his shill, to the for goint \n",
            "awack you sick mans with trungs fless. \n",
            "\n",
            "Sam n \n",
            "\n",
            "[221.28275537490845 (1100 55%) 1.6436]\n",
            "Wh simards when was dristed \n",
            "whelled with back the Ride down at follingled of the for repain. \n",
            "\n",
            "'The s \n",
            "\n",
            "[241.54605555534363 (1200 60%) 2.0170]\n",
            "Whine the fill leep a mught his like I was last all the rove along befues the \n",
            "sone, and all they we h \n",
            "\n",
            "[261.667112827301 (1300 65%) 1.5104]\n",
            "Whing of the \n",
            "\n",
            "\n",
            "\n",
            "But \n",
            "dryward great \n",
            "and \n",
            "your had as him from the pands seay; and to the go the shapt \n",
            "\n",
            "[281.77312326431274 (1400 70%) 1.7484]\n",
            "Wh the reveadil foored though the ever again, \n",
            "he many eveach de. 'No withing \n",
            "said Frodo. 'Enever doa \n",
            "\n",
            "[302.06036829948425 (1500 75%) 1.5148]\n",
            "Wh the has he \n",
            "salk and the greather slet leak out that \n",
            "lighed to firet wos got the \n",
            "gare for \n",
            "the ra \n",
            "\n",
            "[322.32093811035156 (1600 80%) 1.7322]\n",
            "Wh will me, and you gincled dark of the Girned there was a dirist. \n",
            "\n",
            "The Sam. Ore the Ores had paces o \n",
            "\n",
            "[342.47217655181885 (1700 85%) 1.5161]\n",
            "Wh \n",
            "you day of teems. \n",
            "\n",
            "'But a caune isseed,' said Araces.' \n",
            "\n",
            "'Yove silege, I airs it. 'None swill on  \n",
            "\n",
            "[362.7398977279663 (1800 90%) 1.8652]\n",
            "Whouse of last of Elve have begreful \n",
            "horther, a done capplandain and \n",
            "the for the Toring, where it of \n",
            "\n",
            "[383.055038690567 (1900 95%) 1.7744]\n",
            "Whing in for \n",
            "they a did lark agh; Roh dreet. 'Hobbits \n",
            "have in to like \n",
            "soined, the mist, Gandalf. Th \n",
            "\n",
            "[403.20895051956177 (2000 100%) 1.7753]\n",
            "Wh \n",
            "look no laspater. \n",
            "\n",
            "'Uphing ins to the Lary \n",
            "pool park, I aprond fous a pill not world of Smy to s \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dNEe93VcMDrp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Alma"
      ]
    },
    {
      "metadata": {
        "id": "MEQFUfJWLAOb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        },
        "outputId": "62e22d0a-2f1b-42b1-db04-26d2955c46f4"
      },
      "cell_type": "code",
      "source": [
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set(file2))       \n",
        "    loss_avg += loss_\n",
        " \n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate('Wh', 100), '\\n')\n",
        " \n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20.150388956069946 (100 5%) 2.1082]\n",
            "Whis and wore she sam to whe heured in deopt\n",
            " hes of the ano Lour hed Ae hauntis ien hevim we: toreche \n",
            "\n",
            "[39.99161720275879 (200 10%) 1.9285]\n",
            "Wh yere hat been nind God of the Not rouf the purwe Jeord; Tere jeiul the seow kes on meal the God os  \n",
            "\n",
            "[59.98768329620361 (300 15%) 1.7453]\n",
            "Whesd or themy or eveais in whis farts to he sorthre mento behief thor ith loly unto sil thould and ou \n",
            "\n",
            "[79.78358769416809 (400 20%) 1.6565]\n",
            "Whed, of the ling, the sand the hast agaid pak the nore Lead and I sanien liveren be came to saing int \n",
            "\n",
            "[99.60424184799194 (500 25%) 1.5468]\n",
            "Whes are to san of terent abe has belth hear did grefwatiesers and man, and the copCed and did nom thu \n",
            "\n",
            "[119.55217242240906 (600 30%) 1.3159]\n",
            "Whou? I said: And the srought thatious kestore, the Lamanites of a the areir he which hum.\n",
            "\n",
            " And of th \n",
            "\n",
            "[139.45719194412231 (700 35%) 1.5432]\n",
            "Whe mecation, the Lord is the chich ye should nis cass; and hild vaccingech that we lignown, and he sh \n",
            "\n",
            "[159.44428253173828 (800 40%) 1.4866]\n",
            "Whes, behold, ye behold, and becomaning of the Lords, urpon and our exceedinnous that the Lamanitesed  \n",
            "\n",
            "[179.33826732635498 (900 45%) 1.2804]\n",
            "Whoini, and that seecce they wealssssrers which lave that it wigh the peace and because for that fallo \n",
            "\n",
            "[199.26539826393127 (1000 50%) 1.5634]\n",
            "Whe sprity should them did come as the ccame it prisoner of pustifion when where among the Lord were G \n",
            "\n",
            "[219.31997418403625 (1100 55%) 1.3889]\n",
            "Whem, behold, and behold, to have breach their was in the works of the armading them and by you, been  \n",
            "\n",
            "[239.41438364982605 (1200 60%) 1.4873]\n",
            "Whoy strovercemered up my ours, the cimy unto the stood, while coment by the city our to the regent of \n",
            "\n",
            "[259.59244298934937 (1300 65%) 1.4641]\n",
            "Whover of the nod this man out bording of the land on; their desterong of the eart will into the wor h \n",
            "\n",
            "[279.7010269165039 (1400 70%) 1.4961]\n",
            "Whorants that they forth a commandsters we that Ammon to the people the word of they been that they di \n",
            "\n",
            "[299.9533956050873 (1500 75%) 1.2884]\n",
            "Whe said:\n",
            "\n",
            " Yefored, and the ong to had a concernist and beholt unto them desirit, even about said bee \n",
            "\n",
            "[320.4077925682068 (1600 80%) 0.9846]\n",
            "Wht and all me to am thot spartile, when thought of repented; and weapons, for Alma nown the resare th \n",
            "\n",
            "[341.6113488674164 (1700 85%) 1.1120]\n",
            "Whedm to all the broth much words and innentende that has blood that the king with was of Nephites, an \n",
            "\n",
            "[363.2595965862274 (1800 90%) 1.2534]\n",
            "Whum's of his prolick of the commands their fall, and quent of Muld, and his repented angent of the ri \n",
            "\n",
            "[384.1828873157501 (1900 95%) 1.2573]\n",
            "Whor should the came that in the fest, yea, and they much their according the game to pass that the La \n",
            "\n",
            "[404.68363070487976 (2000 100%) 0.9701]\n",
            "Whought to ssoice it them had be repento the mo in as the Lord should sland.\n",
            "\n",
            " Now the fourge, and mee \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M6X1Nlj9MGR7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Taylor Swift lyrics"
      ]
    },
    {
      "metadata": {
        "id": "dd_aZrwRMFhC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2009
        },
        "outputId": "442e57db-0799-41f5-9f8b-e6cdae220acf"
      },
      "cell_type": "code",
      "source": [
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        " \n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set(file3))       \n",
        "    loss_avg += loss_\n",
        " \n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate('Wh', 100), '\\n')\n",
        " \n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20.435955286026 (100 5%) 2.3350]\n",
            "Whoor sow'ind yow wh oo somy kis whash,\n",
            "Ang you lay kiper ho rinl gachare barit hathinl wou baster fiw \n",
            "\n",
            "[40.99764823913574 (200 10%) 1.9893]\n",
            "Whe syou he the sae in the say's\n",
            "You rere sthy, stayot to koung yust\n",
            "\n",
            "But\n",
            "Buthe nowe you like sta ghow \n",
            "\n",
            "[61.663105726242065 (300 15%) 2.1588]\n",
            "Wher ong\n",
            "That arat reand\n",
            "Shour bere sorow, to a un tere sime\n",
            "2n in prous sole, on't of and\n",
            "And tere se \n",
            "\n",
            "[82.06050777435303 (400 20%) 1.8977]\n",
            "Whing sher, sich shownt way to and Ip thired?\n",
            "I waing this a for whond wails am sall dimed you'd eve p \n",
            "\n",
            "[102.67081117630005 (500 25%) 1.9534]\n",
            "Whe cat whe do betrout where wer a well laick a got with anrerout you do back the way faldy some...... \n",
            "\n",
            "[123.15146708488464 (600 30%) 1.7862]\n",
            "Whing we al in \n",
            "I night to feld enough your hor can night it coom\n",
            "All you love it's gong\n",
            "You in to you \n",
            "\n",
            "[143.59419107437134 (700 35%) 1.7957]\n",
            "Wher am baby, I want it got fase\n",
            "I (xchemm was of in was us flore your haget a rlay\n",
            "I love you wanna I \n",
            "\n",
            "[163.97365188598633 (800 40%) 1.6946]\n",
            "What your for yes\n",
            "Kpell this loved the were all and down\n",
            "So for the dant that firna karday for the sin \n",
            "\n",
            "[184.38924312591553 (900 45%) 1.6842]\n",
            "When just wores a fate\n",
            "Ame time we's teld you\n",
            "\n",
            "Way I knows aloress\n",
            "We foring a fave the hand\n",
            "And I've  \n",
            "\n",
            "[204.71335291862488 (1000 50%) 1.5682]\n",
            "What you, to to they on a get you\n",
            "\n",
            "Way, she's the rughing what you back there not?\n",
            "I was a was gfont a \n",
            "\n",
            "[225.25376677513123 (1100 55%) 1.5284]\n",
            "Whing be much\n",
            "Don't the tell I left a say we gone\n",
            "\n",
            "Swouck mllever, downng stron't bad\n",
            "\n",
            "I thore time th \n",
            "\n",
            "[245.688570022583 (1200 60%) 1.7058]\n",
            "Whing tome\n",
            "\n",
            "It's all me to reaming\n",
            "\n",
            "\n",
            "I kift everybody been, \"Yess and to stard feelis hat light, surn\n",
            " \n",
            "\n",
            "[266.1518876552582 (1300 65%) 1.7514]\n",
            "Wh\n",
            "You some it a love to me emp mad\n",
            "And I watck a mount so new you ol, yourlress\n",
            "You're and eyes out i \n",
            "\n",
            "[287.08621168136597 (1400 70%) 1.6592]\n",
            "Why and Salk fils and your from\n",
            "But you what eyes\n",
            "They look in year to foreming\n",
            "To girls in you downed \n",
            "\n",
            "[308.24804878234863 (1500 75%) 1.5874]\n",
            "What you say\n",
            "I gonna have it wanna dream\n",
            "Sone't to have to wCat your thould the saids leave for that?\n",
            " \n",
            "\n",
            "[329.1328663825989 (1600 80%) 1.2827]\n",
            "Where it's gonna and your baby\n",
            "And in what you and the said, the you I could and\n",
            "When the way the side \n",
            "\n",
            "[349.8931703567505 (1700 85%) 1.3844]\n",
            "When you're baby So mad\n",
            "\n",
            "I some and about your cause it wanna and loving of a time, around what hurly  \n",
            "\n",
            "[370.8089482784271 (1800 90%) 1.4286]\n",
            "Where wors and the must where still are\n",
            "The prother the sudna be wind you\n",
            "It all the wind the triever  \n",
            "\n",
            "[391.44746136665344 (1900 95%) 1.4359]\n",
            "Whout sown\n",
            "In my chooss just all of so the thap\n",
            "In was lifew you with thell nevirds, that you cause it \n",
            "\n",
            "[412.3830289840698 (2000 100%) 1.4052]\n",
            "When you walked to don't were.\n",
            "\n",
            "The dray neman't had want the monat put the wilder\n",
            "And mate ie mide wh \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3O6k83q4PkMc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Sense and Sensibility"
      ]
    },
    {
      "metadata": {
        "id": "RHMPuREbPniU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        },
        "outputId": "6dc11409-106f-4697-860a-f6d369e1b773"
      },
      "cell_type": "code",
      "source": [
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        " \n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    loss_ = train(*random_training_set(file4))       \n",
        "    loss_avg += loss_\n",
        " \n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "        print(evaluate('Wh', 100), '\\n')\n",
        " \n",
        "    if epoch % plot_every == 0:\n",
        "        all_losses.append(loss_avg / plot_every)\n",
        "        loss_avg = 0"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20.71542763710022 (100 5%) 2.2608]\n",
            "Wh thidilaug he. athirI be whe whe ave he house erte lobe th thiser7son evg mior whed das was \"hordee  \n",
            "\n",
            "[41.477357149124146 (200 10%) 2.0774]\n",
            "Whe she in to foran has of her yond of hat hind hat co could tas and rish oup hat in an a was ath an b \n",
            "\n",
            "[62.25443077087402 (300 15%) 1.9960]\n",
            "Wh. As she prou her all reashelind worice Ture. Nhenr was wuch in vere fnenfore wion in-lito her mand  \n",
            "\n",
            "[83.31416916847229 (400 20%) 1.6512]\n",
            "Wh the thougome od of halnaint for that his as leithald ir esparle and prom me the the persant he enou \n",
            "\n",
            "[103.97436022758484 (500 25%) 1.7920]\n",
            "Whadune, the give shat of than, in unser where? he proponsr to pichwood Mothery in the at them, be may \n",
            "\n",
            "[124.57213068008423 (600 30%) 1.6749]\n",
            "Wh, fortate was may, withen othee atget my her simably as Maris-in her mich to house's theart as in th \n",
            "\n",
            "[145.23896050453186 (700 35%) 1.7579]\n",
            "Wh were of be with semely at ald and she mowifle of exceenession I deade her sel hat same the lome, hi \n",
            "\n",
            "[166.2514135837555 (800 40%) 1.5259]\n",
            "Wh, him to have of A what what I hay and, her sater of in there to be suftion of it partay consisuase, \n",
            "\n",
            "[186.46706628799438 (900 45%) 1.5750]\n",
            "Wh, and I know would it eaghing that he will de, her all that she letter's propence his for to belal t \n",
            "\n",
            "[206.96479725837708 (1000 50%) 1.7256]\n",
            "Whn man not behave of and sumprever were had somether. It was to the reasher approst in thing was is i \n",
            "\n",
            "[227.4866006374359 (1100 55%) 1.2772]\n",
            "Wh! Mosal to be some the evey recure. She his ot the rectainly. No they firly of they wrotible; bt it  \n",
            "\n",
            "[247.94429445266724 (1200 60%) 1.5366]\n",
            "Wh a did not a could her far fortent of his were amand a sime by incommonted in his lecardince with se \n",
            "\n",
            "[268.49820017814636 (1300 65%) 1.5129]\n",
            "WhL CLAPinor scarctance of his was comport gromed, hear stilly be more as firrited oh plate had sens.  \n",
            "\n",
            "[288.8981354236603 (1400 70%) 1.1901]\n",
            "Whand the truat a-year she could him pounds daughter than you be incompy insome, I to hose what, not b \n",
            "\n",
            "[309.3955111503601 (1500 75%) 1.5332]\n",
            "Wh to her a ncage, which distance to way the give his partiaty of annuition, in Devonsom, he was decem \n",
            "\n",
            "[329.9530041217804 (1600 80%) 1.4222]\n",
            "What he was too ould not be consideration was had good feeling in a side sister. She muce estle, was h \n",
            "\n",
            "[350.4916932582855 (1700 85%) 1.2179]\n",
            "What happly any pressence in are by for at I liken than she all his somes, an unure a pleasing of not  \n",
            "\n",
            "[371.0977668762207 (1800 90%) 1.3323]\n",
            "Wh might of his might intentions of the should and as into Devonst sefulation of could, for to sention \n",
            "\n",
            "[391.542995929718 (1900 95%) 1.4421]\n",
            "Wh which in so more man not properturation to higher would geempers, what deveryt letter to him husbab \n",
            "\n",
            "[412.1708810329437 (2000 100%) 1.4011]\n",
            "Wh with should be marked possenser the dral she with some laye can orten. He didefticulor she wished b \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oZdddsew9q_q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}